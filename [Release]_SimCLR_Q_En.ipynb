{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-k_bJg7BWDBT"
   },
   "source": [
    "# Goal:\n",
    "\n",
    "In this assignment, you will implement the self-supervised contrastive learning algorithm, [SimCLR](https://arxiv.org/abs/2002.05709), using PyTorch. You will use the STL-10 dataset for this assignment.\n",
    "\n",
    "You need to compete the `Net` class definition, the `SimCLRDataset` dataset class definition and the SimCLR loss in the `Trainer` class. You need to run the training loop, save the best training model and evaluate using the `linear probe` classfication task. Since we don't have enough GPU resources and contrastive learning algorithm like SimCLR usually needs around `1000` epoches to train (we only have `70` epoches), you may not get the best performance. Thus, as for the performance side, as long as you see the loss is decreasing (to around 7.4 at `70` epoch) and the accuracy is increasing, you are good to go.\n",
    "\n",
    "Grade:\n",
    "\n",
    "- **Fill the Net class definition (5 points).**\n",
    "- **Fill the SimCLRDataset dataset class definition (10 points).**\n",
    "- **Fill the SimCLR loss in the Trainer class (20 points).**\n",
    "- **Record the training loss within 70 epochs, the lower the better (5 points).**\n",
    "- **Record the linear probe accuracy, the higher the better (5 points).**\n",
    "- **Write a report including:**\n",
    "  - **How you select data augmentation (transform) in the transform pool.**\n",
    "  - **How you implement the SimCLR loss and explain why your SimCLR loss is computationally efficient and equivalent to the loss function in the paper.**\n",
    "  - **Include the training loss curve and the downstream accuracy (15 points). Note that the logging logic is not provided, please implement them before you start training.**\n",
    "---\n",
    "Please DO NOT change the config provided. Only change the given code if you are confident that the change is necessary. It is recommended that you **use CPU session to debug** when GPU is not necessary since Colab only gives 12 hrs of free GPU access at a time. If you use up the GPU resource, you may consider to use Kaggle GPU resource. Thank you and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TSjaMEiemQe"
   },
   "source": [
    "# Self-supervised learning: SimCLR\n",
    "\n",
    "Self-supervised learning\n",
    "\n",
    "1.   Design an auxiliary task.\n",
    "2.   Train the base network on the auxiliary task.\n",
    "3.   Evaluate on the down-stream task: Train a new decoder based on the trained encoder.\n",
    "\n",
    "Specifically, as one of the most successful self-supervised learning algorithm, SimCLR, a contrastive learning algorithm is what we focus today. Below, we are going to implement SimCLR as an example of self-supervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ocq-13HDLET"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/35af3432fbe91c56a934b5ee58931b4848ab35043830c9dd6f08fa41e6eadbe7/68747470733a2f2f312e62702e626c6f6773706f742e636f6d2f2d2d764834504b704539596f2f586f3461324259657276492f414141414141414146704d2f766146447750584f79416f6b4143385868383532447a4f67457332324e68625877434c63424741735948512f73313630302f696d616765342e676966\" width=\"650\" height=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtZk2Wb_V7Z7"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "# Since, we are using jupyter notebook, we use easydict to micic argparse. Feel free to use other format of config\n",
    "from easydict import EasyDict\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "config = {\n",
    "    'dataset_name': 'stl10',\n",
    "    'workers': 1,\n",
    "    'epochs': 70,\n",
    "    'batch_size': 3072,\n",
    "    'lr': 0.0003,\n",
    "    'weight_decay': 1e-4,\n",
    "    'seed': 4242,\n",
    "    'fp16_precision': True,\n",
    "    'out_dim': 128,\n",
    "    'temperature': 0.5,\n",
    "    'n_views': 2,\n",
    "    'device': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "args = EasyDict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6fxqKRifbgC"
   },
   "source": [
    "We are going to use [STL-10 dataset](https://cs.stanford.edu/~acoates/stl10/).\n",
    "\n",
    "<img src=\"https://cs.stanford.edu/~acoates/stl10/images.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "Overview\n",
    "\n",
    "*   10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.\n",
    "*   Images are **96x96** pixels, color.\n",
    "*   500 training images (10 pre-defined folds), 800 test images per class.\n",
    "*   100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.\n",
    "*   Images were acquired from labeled examples on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D9x3PGGlwQO"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Define a ResNet-18 and an additional MLP layer as the model training in the auxiliary task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNUQ1b8lp68n"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.basemodel = models.resnet18(pretrained=False, num_classes=args.out_dim)\n",
    "        self.fc_in_features = self.basemodel.fc.in_features\n",
    "        self.backup_fc = None\n",
    "        # ToDO: define an MLP layer to insert between the last layer and the rest of the model\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ToDo: implement the forward logic\n",
    "        ...\n",
    "\n",
    "    def linear_probe(self):\n",
    "        self.freeze_basemodel_encoder()\n",
    "        self.backup_fc = self.basemodel.fc  # Backup the last Linear layer\n",
    "        # ToDo: implement the linear probe for your downstream task. A linear prob is just a linear layer (not MLP, no activation layer included) after the learned encoder.\n",
    "        self.basemodel.fc = ...\n",
    "\n",
    "    def restore_backbone(self):\n",
    "        self.basemodel.fc = self.backup_fc\n",
    "        self.backup_fc = None\n",
    "\n",
    "    def freeze_basemodel_encoder(self):\n",
    "        # do not freeze the self.basemodel.fc weights\n",
    "        for name, param in self.basemodel.named_parameters():\n",
    "            if 'fc' not in name:\n",
    "                param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKxWA4tDg9vE"
   },
   "source": [
    "# Step 1: Design the auxiliary task.\n",
    "## construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg61k4QVfa2_"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "\n",
    "class View_sampler(object):\n",
    "    \"\"\"This class randomly sample two transforms from the list of transforms for the SimCLR to use. It is used in the SimCLRDataset.get_dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, transforms, n_views=2):\n",
    "        self.transforms = transforms\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transforms(x) for i in range(self.n_views)]\n",
    "\n",
    "\n",
    "class SimCLRDataset:\n",
    "    def __init__(self, root_folder=\"./datasets\"):\n",
    "        self.root_folder = root_folder\n",
    "\n",
    "    @staticmethod\n",
    "    def transforms_pool(...):\n",
    "        # ToDo\n",
    "        data_transforms = transforms.Compose([...]) # for example, you can use RandomResizedCrop, RandomHorizontalFlip, ColorJitter, RandomGrayscale, GaussianBlur, ToTensor (compulsory), Normalize (), etc.\n",
    "        return data_transforms\n",
    "\n",
    "    def get_dataset(self):\n",
    "        dataset_fn = lambda: datasets.STL10(self.root_folder, split='unlabeled', transform=View_sampler(self.transforms_pool(...), 2), download=True)\n",
    "        return dataset_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTHLDeCalsbx"
   },
   "source": [
    "## Define dataloader, optimizer and scheduler\n",
    "\n",
    "What is a scheduler?\n",
    "\n",
    "A scheduler helps in optimizing the convergence, avoiding local minima, and potentially improving the model's performance on the task at hand. The learning rate is one of the most important hyperparameters for training neural networks, and finding an appropriate learning rate schedule can be crucial for your model's success.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qe6nYlH8zsmUdScyHMhRCQ.png\" width=\"1200\" height=\"450\">\n",
    "\n",
    "Read more here: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpMp8XB3lma1"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "dataset = SimCLRDataset()\n",
    "train_dataset = dataset.get_dataset(args.dataset_name, args.n_views)\n",
    "# ToDo, define dataloader based on the train_dataset with drop_last=True\n",
    "dataloader = ...\n",
    "# ToDo, define an optimizer with args.lr as the learning rate and args.weight_decay as the weight_decay\n",
    "optimizer = ...\n",
    "# ToDo, define an lr_scheduler CosineAnnealingLR for the optimizer\n",
    "lr_scheduler = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBeD23fj0UdR"
   },
   "source": [
    "# Define the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrEbwApkCnVc"
   },
   "source": [
    "\n",
    "Automatic Mixed Precision (AMP) is a technique that aims to improve the speed and efficiency of training deep neural networks by leveraging mixed-precision training.\n",
    "\n",
    "**Introduction to AMP**\n",
    "\n",
    "AMP allows neural network training to use both single-precision (FP32) and half-precision (FP16) floating point arithmetic simultaneously. The main idea behind AMP is to perform certain operations in FP16 to exploit the faster arithmetic and reduced memory usage of lower-precision computing, while maintaining the critical parts of the computation in FP32 to ensure model accuracy and stability.\n",
    "\n",
    "**Why We Cannot Always Use FP16**\n",
    "\n",
    "\n",
    "\n",
    "*   **Numerical Stability**: FP16 has a smaller dynamic range and lower precision compared to FP32. This limitation can lead to numerical instability, such as underflows and overflows, particularly during operations that involve small gradient values or require high numerical precision. This can adversely affect the convergence and accuracy of the trained model.\n",
    "*   **Selective Precision Requirements**: Certain operations and layers within neural networks are more sensitive to precision than others. For example, weight updates in optimizers might require FP32 to maintain accuracy over time. AMP strategies, therefore, involve selectively applying FP16 to parts of the computation where it can be beneficial without undermining the overall training process.\n",
    "\n",
    "Below, we present how to include AMP logic in standard torch training procedure.\n",
    "\n",
    "Before including AMP:\n",
    "```python\n",
    "for batch in data_loader:\n",
    "    # Forward pass\n",
    "    inputs, targets = batch\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "After including AMP:\n",
    "```python\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for batch in data_loader:\n",
    "    inputs, targets = batch[0].cuda(), batch[1].cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "\n",
    "Read more here: https://pytorch.org/docs/stable/amp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBOPRrzsemQi"
   },
   "source": [
    "## Implement loss function in the Trainer\n",
    "The algorithm of SimCLR Loss function is as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{for all } i \\in \\{1, \\ldots, 2N\\} \\text{ and } j \\in \\{1, \\ldots, 2N\\} \\text{ do} \\\\\n",
    "&\\quad s_{i,j} = \\frac{z_i^\\top z_j}{\\|z_i\\|\\|z_j\\|} \\quad \\text{# pairwise similarity} \\\\\n",
    "&\\text{end for} \\\\\n",
    "&\\text{define } \\ell(i,j) \\text{ as } \\ell(i,j) = -\\log \\left( \\frac{\\exp(s_{i,j} / \\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\neq j]} \\exp(s_{i,k} / \\tau)} \\right) \\\\\n",
    "&\\mathcal{L} = \\frac{1}{2N} \\sum_{k=1}^{N} \\left[\\ell(2k-1, 2k) + \\ell(2k, 2k-1)\\right] \\\\\n",
    "&\\text{update networks } f \\text{ and } g \\text{ to minimize } \\mathcal{L}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Please fill the blanks in the loss function below. Hint: implement mask to avoid including self-self similarity, and postive pairs and negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo_7-Eop0T4Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = kwargs['args']\n",
    "        self.model = kwargs['model'].to(self.args.device)\n",
    "        self.optimizer = kwargs['optimizer']\n",
    "        self.scheduler = kwargs['scheduler']\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.args.device)\n",
    "\n",
    "    def loss(self, features):\n",
    "        # input features is a torch tensor with shape of (2*batch_size, out_dim)\n",
    "        # The positive pairs are (features[i] and features[i+batch_size]) for all i\n",
    "        # TODO: implement the loss function\n",
    "        return loss\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        # implement GradScaler if AMP\n",
    "        best_loss = 1e4\n",
    "        scaler = GradScaler(enabled=self.args.fp16_precision)\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for images, _ in tqdm(dataloader):\n",
    "                images = torch.cat(images, dim=0)\n",
    "                images = images.to(self.args.device)\n",
    "\n",
    "                with autocast(enabled=self.args.fp16_precision):\n",
    "                    features = self.model(images)\n",
    "                    loss = self.loss(features)\n",
    "\n",
    "                # features = self.model(images)\n",
    "                # loss = self.loss(features)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # loss.backward()\n",
    "                # self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # warmup for the first 10 epochs\n",
    "            if epoch >= 10:\n",
    "                self.scheduler.step()\n",
    "            if epoch % 10 == 0 and epoch != 0:\n",
    "                self.save_model(self.model, f\"model_{epoch}.pth\")\n",
    "            # save the lowest loss model\n",
    "            # feel free to implement your own logic to save the best model\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                self.save_model(self.model, f\"best_model.pth\")\n",
    "            print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self, model, path):\n",
    "        torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrqKVgUeFfM8"
   },
   "source": [
    "# Step 2: Train the base network on the auxiliary task for 70 epoch and save the best model you have for evaluation.\n",
    "\n",
    "Check if the training loss drops over time and try to capture other possible bug using logging tools. Each epoch should take around 7 minutes. The loss should be expected to around 7.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keYUDQ7aFx6x"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(args=args, model=model, optimizer=optimizer, scheduler=lr_scheduler)\n",
    "trainer.train(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0WWUzP5emQj"
   },
   "source": [
    "# Step 3: Evaluate on the down-stream task: Train a new MLP decoder based on the trained encoder.\n",
    "\n",
    "This finetune process should be way faster than the previous one. The expected Top-1 accuracy should be around 57% and the Top-5 accuracy should be around 97%. Getting this results is normal because linear prob is just a projection layer usually recognized as not having representation ability. To achieve the performance mentioned in the paper, we need larger dataset and more powerful GPU and longer time (about 1000 epoches during the pretraining stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R2HGu-UemQj"
   },
   "outputs": [],
   "source": [
    "class linear_prob_Trainer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = kwargs[\"args\"]\n",
    "        self.model = kwargs[\"model\"].to(self.args.device)\n",
    "        self.optimizer = kwargs[\"optimizer\"]\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.args.device)\n",
    "        self.train_dataset = datasets.STL10(\n",
    "            \"./data\", split=\"train\", download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            num_workers=1,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        self.test_dataset = datasets.STL10(\n",
    "            \"./data\", split=\"test\", download=True, transform=transforms.ToTensor()\n",
    "        )\n",
    "\n",
    "        self.test_loader = torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.args.batch_size,\n",
    "            num_workers=1,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def accuracy(self, output, target, topk=(1,)):\n",
    "        with torch.no_grad():\n",
    "            maxk = max(topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            res = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            return res\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        for epoch in range(100):\n",
    "            top1_train_accuracy = 0\n",
    "            for images, labels in tqdm(dataloader):\n",
    "                images, labels = images.to(self.args.device), labels.to(\n",
    "                    self.args.device\n",
    "                )\n",
    "                logits = self.model(images)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                top1 = self.accuracy(logits, labels, topk=(1,))\n",
    "                top1_train_accuracy += top1[0]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss.item()}\",\n",
    "                    \"Top1 Train Accuracy: \",\n",
    "                    top1_train_accuracy.item() / len(dataloader),\n",
    "                )\n",
    "        return self.model\n",
    "\n",
    "    def test(self, dataloader):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            top1_test_accuracy = 0\n",
    "            top5_test_accuracy = 0\n",
    "            for images, labels in tqdm(dataloader):\n",
    "                images, labels = images.to(self.args.device), labels.to(\n",
    "                    self.args.device\n",
    "                )\n",
    "                logits = self.model(images)\n",
    "                top1 = self.accuracy(logits, labels, topk=(1,))\n",
    "                top1_test_accuracy += top1[0]\n",
    "                top5 = self.accuracy(logits, labels, topk=(5,))\n",
    "                top5_test_accuracy += top5[0]\n",
    "            print(\"Top1 Test Accuracy: \", top1_test_accuracy.item() / len(dataloader))\n",
    "            print(\"Top5 Test Accuracy: \", top5_test_accuracy.item() / len(dataloader))\n",
    "            return\n",
    "# load your model here if you need to resume\n",
    "# ...\n",
    "model.linear_probe()\n",
    "linear_probe_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "linear_prob_trainer = linear_prob_Trainer(args=args, model=model, optimizer=linear_probe_optimizer)\n",
    "model = linear_prob_trainer.train(trainer.train_loader)\n",
    "linear_prob_trainer.test(trainer.test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
